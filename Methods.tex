%\vspace{-0.5cm}

\section{Methods} \label{sec:methods} 
\vspace{-0.15cm}
    \subsection{Data acquisition and processing}\label{sec:dataproc}
        Dynamic \gls{PBR28} \gls{PET}/\gls{MR} images from $52$ individuals (Age: $55 \pm 16$ years; Sex: $27$ Male, $25$ Female; Genotype: $32$ \glspl{HAB}, $20$ \glspl{MAB}; Clinical population: $12$ Healthy Controls, $40$ Chronic Pain patients; Injected Dose: $14.16 \pm 1.3$ \glspl{mCi}) were acquired on a Siemens Biograph mMR whole-body tomograph for a time-period of $0$-$90$ minutes post-injection. Data were pooled for multiple protocols (approved by the Partners Healthcare/Mass General Brigham Institutional Review Board) and reconstructed as in \cite{Brusaferri2022ThePandemic}. All subjects received a radial artery catheter at the time of the scan. Uncorrected plasma curves from the blood samples were interpolated and metabolite-corrected to obtain the \gls{AIF}. %Uncorrected plasma curves were obtained from raw blood samples using linear fitting. In $16$ subjects, arterial blood processing was performed using a HyperSep C$18$ solid extraction cartridge to separation of radio-metabolites; in $36$ subjects, \gls{HPLC} for separation of radio-metabolites from parent radiotracer was used instead. Hill-fitted parent fractions were applied to the raw data and a radio-metabolite-corrected \gls{AIF} was obtained for each subject. Previous cross-validation confirmed reliability of both pipelines allowing for the combination of both data-sets to increase statistical power of the study~\cite{Brusaferri2022ThePandemic}. 
        
        For validation purposes, \gls{IDIF} was also calculated by segmenting the arterial carotid siphons via intensity thresholding of early  dynamic \gls{PET} frames. Data were split following ten-fold cross validation. The split was specifically designated to maximise the within-variance while minimising the between-variance in training and testing sets.

    \vspace{-0.5cm}
    
    \subsection{Neural Network Design}\label{sec:NNDesign}
        The method comprises of three independent \glspl{NN}: \gls{NN}$_1$ seeks to reduce the dimentionality of the input data (due to computational requirements) and extract the most relevant features. \gls{NN}$_2$ aims to extract a non-metabolite corrected signal from the low-dimensional representation output by the first network. \gls{NN}$_3$ metabolite corrects the non-metabolite corrected signal. Details follow in the next sections.
        %All models use a novel activation function, defined as $PSoftplus = \log\big(\mathrm{e}^{x} + \alpha^2 \big)$, where $x$ is the output from the previous layer and $\alpha$ is a learnt parameter which is initialised as $\alpha = 0$.  This activation function was designed as a fully differentiable drop-in replacement for \textit{PReLU} \cite{Ciuparu2020Soft++Architectures}. The initial value of $\alpha$ is selected such that the activation is initially linear (thus making the model easier to train at early iterations) and becomes non-linear as training progresses.
        % https://www.sciencedirect.com/science/article/pii/S0925231219317163

        %In order to try to enforce disentanglement and continuity of this layer, a regularisation term to enforce the orthogonality of its output was used. To promote stability of the optimisation, a regularisation term which compares the area under the curve of the prediction and true signals was used in each \gls{NN}. Common to all \glspl{NN}, each model was optimised using AdaBelief~\cite{Zhuang2020AdaBeliefGradients} with the warm-up proportion equivalent to one-tenth of the total number of epochs, after which it does not decrease. Weight decay was used to regularise against large weights. %Each model was trained initially using \gls{RMSE}, where the mean output of the model is penalised against the expected value and the standard deviation output of the model is encouraged to be close to $1$. Subsequently, a second training regime was used where the negative \gls{LL} function is used to fine-tune the parameters. Gradient accumulation was used to allow for only one data-point to be loaded onto the \gls{GPU} at any one time and for any arbitrary batch-size to be used. The batch size starts at two and increases to the size of total number of data points, where the batch size doubles as the current epoch number quadruples.

        \subsubsection{\gls{NN}$_1$ \gls{AE}} \label{sec:NN1}
            This network consists of three blocks, the downsampling block, the latent layer, and the upsampling block. The downsampling block comprises three convolutions, all with kernel size equal to three, and where the final convolution has a stride of two. The latent block is flanked on either side by two convolutions, with a variational latent layer in the middle. The upsampling block consists of a transposed convolution and two standard convolutions, all with kernel size equal to three, and where the transposed convolution has a stride of two. The mean and standard deviation of the latent layer and final layer are output from the model and passed onto \gls{NN}$_2$.
            Each time frame was treated as an independent training example.

            %The target images were smoothed using a Gaussian filter with a \gls{FWHM} equal to three times the voxel size in each dimension. All images were padded such that the size of each dimension was equal to the next larger power of two. Both input and target images were standardised separately, based on parameters obtained from the training set.

        \subsubsection{\gls{NN}$_2$ Signal Extractor} \label{sec:NN2}
            This network consists of three blocks, the downsampling block, the latent layer and the fully connected block. The downsampling block follows the same structure as in \gls{NN}$_1$. The fully connected block consists solely of one fully connected layer. All time frames were used simultaneously, where the same convolutions are applied independently on the image of each time frame before global average pooling and flattening. Both input and target data were standardised separately, based on parameters obtained from the training set. 
        \subsubsection{\gls{NN}$_3$ Metabolite Correction and Reshaping} \label{sec:NN3}
            This network consists solely of fully connected layers. If the network is to metabolite-correct a signal (e.g. from \gls{AIF} or \gls{IDIF}), it takes that signal as input together with the clinical features (age, sex, genotype, injected dose, clinical population).  If the network is instead to correct a signal obtained with \gls{NN}$_2$, both the mean and the standard deviation of the uncorrected signal are input to \gls{NN}$_3$, in addition to the latent layer from \gls{NN}$_2$. %The input data was preprocessed in the same way as for \gls{NN}$_2$.

            %Both input and target data are standardised separately, based on parameters obtained from the training set. Only the mean was standardised. Each element of the clinical features was either standardised or encoded using one hot encoding depending on the nature of the feature. Because the autoencoder outputs both a mean and a standard deviation values were sampled at the input of the network using the same \textit{reparametrisation-trick} as for a variational autoencoder.

    \vspace{-0.44cm}
    
    \subsection{Evaluation}\label{sec:evaluation}
    \vspace{-0.1cm}
        %The mean and standard deviation were sampled $32$ times (to satisfy the central limit theorem) from the model and the output was sampled $32$ times from the mean of these to acquire multiple realisations of the estimated signal. 
        %Then, candidate signals from~\Fref{sec:candidates} were compared in terms of \glspl{VT}, calculated via Logan graphical method, for each realisation of the output (\Fref{sec:metrics}), in $69$   \glspl{ROI}.

        \subsubsection{Metrics} \label{sec:metrics}
         The model was sampled $32$ times resulting in multiple realisations of the estimated signal. Then, $V_{\mathrm{T}} \in \mathbb{R}^{r \times s \times b}$ were computed via Logan graphical method, where $r$ is the number of \glspl{ROI} ($r = 69$), $s$ is the number of subjects ($s = 5$) and $b$ is the number of model samples ($b = 32$). For both AIF and IDIF, $b=1$. For the \gls{NN}-based methods, \glspl{VT} were computed for each model realisation and then used to calculate the mean \gls{VT} and its standard deviation.
        Moreover, the \gls{CV} was defined as $\mathrm{CV} = \mathrm{std} ( Pred \; V_{\mathrm{T}} ) / True \; V_{\mathrm{T}}$ with $\mathrm{CV} \in \mathbb{R}^{r \times s }$.

        \subsubsection{Statistical Analyses} \label{sec:stats}
        For each candidate signal, correlation analyses were performed to compare the \glspl{VT} (computed for all the \glspl{ROI} and for all the subjects in the test-set) to the ones obtained with the ground truth signal (\textbf{TRUE-AIF}, see~\Fref{sec:dataproc}). To measure the accuracy of the prediction, the angle between the regression- and the identity-line was also computed, defined as $\theta = 45 - \arctan(m)*180/\pi$, with $m$ being the slope of the regression-line.
        Furthermore, \glspl{CV} were averaged across \glspl{ROI} for each subject of the test-set and compared via a paired t-test for each of the three \gls{NN}-based methods.
       
        \subsubsection{Candidate signals} \label{sec:candidates}
            \begin{itemize}
                \item \textbf{\gls{NN}-\gls{AIF}} - metabolite-corrected \gls{AIF} obtained from (uncorrected) arterial plasma input to \gls{NN}$_3$~\Fref{sec:NN3}.
                \item \textbf{\gls{NN}-\gls{AE}\gls{IF}} - metabolite-corrected \gls{IF} obtained from dynamic \gls{PET} images input to \gls{NN}$_{1,2,3}$~\Fref{sec:NN1} - \Fref{sec:NN3}.
                \item \textbf{\gls{NN}-\gls{IDIF}} - metabolite-corrected \gls{IF} obtained from \gls{IDIF} input to \gls{NN}$_3$~\Fref{sec:NN3}.
                \item  \textbf{\gls{IDIF}} - generated as in~\Fref{sec:dataproc}.
            \end{itemize}
