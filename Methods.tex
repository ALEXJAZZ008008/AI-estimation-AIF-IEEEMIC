%\vspace{-0.5cm}

\section{Methods} \label{sec:methods}    
    \subsection{Data acquisition and processing}\label{sec:dataproc}
        Dynamic \gls{PBR28} \gls{PET}/\gls{MR} images from $52$ individuals (Age: $55 \pm 16$ years; Sex: $27$ Male, $25$ Female; Genotype: $32$ \glspl{HAB}, $20$ \glspl{MAB}; Clinical population: $12$ Healthy Controls, $40$ Chronic Pain patients; Injected Dose: $14.16 \pm 1.3$ \glspl{mCi}) were acquired on a Siemens Biograph mMR whole-body tomograph for a time-period of $0$-$90$ minutes post-injection~\cite{Delso2011PerformanceScanner}. Data were pooled for multiple protocols (approved by the Partners Healthcare/Mass General Brigham Institutional Review Board).  All participants gave written informed consent at the time of their screening and were genotyped for the Ala$147$Thr polymorphism in the \gls{TSPO} gene, which is know to affect binding affinity for \gls{PBR28} (ref). Subjects were injected with up to approximately $15$ \glspl{mCi} \gls{PBR28} as an intravenous bolus and dynamic \gls{PET} was acquired and reconstructed as described in previous studies~\cite{Alshelh2020In-vivoIllness}.
        
        Structural T$1$ images were acquired for the purpose of anatomical localisation, spatial normalisation of \gls{PET} data, and generation of \gls{AC} maps. \gls{SUV} \gls{PET} images were reconstructed on the mMR using default settings: \gls{OSEM} with four iterations, $21$ subsets, and a \SI{3}{\milli\meter} \gls{FWHM} Gaussian smoothing.
        
        All subjects received a radial artery catheter at the time of the scan. Blood samples were collected at $5$, $10$, $20$, $30$, $50$, $70$, and $90$ minutes post radiotracer injection. Uncorrected plasma curves were obtained from raw blood samples using linear fitting. In $16$ subjects, arterial blood processing was performed using a HyperSep C$18$ solid extraction cartridge to separation of radio-metabolites; in $36$ subjects, \gls{HPLC} for separation of radio-metabolites from parent radiotracer was used instead. Hill-fitted parent fractions were applied to the raw data and a radio-metabolite-corrected \gls{AIF} was obtained for each subject. Previous cross-validation confirmed reliability of both pipelines allowing for the combination of both data-sets to increase statistical power of the study~\cite{Brusaferri2022ThePandemic}. 
        
        For validation purposes, \gls{IDIF} was also calculated by segmenting the arterial carotid siphons via intensity thresholding of early  dynamic \gls{PET} frames, and averaging the \glspl{TAC} of \SI{70}{\percent} of voxels within the so-defined \gls{ROI} with the most highly correlated dynamics.

        Data were split following ten-fold cross validation. The split was specifically designated to maximise the within- while minimising the between-variance in training and testing sets.

    \vspace{-0.5cm}
    
    \subsection{Neural Network Design}\label{sec:NNDesign}
        The method comprises of three independent \glspl{NN}: \gls{NN}$_1$ seeks to reduce the dimentionality of the input data (due to computational requirements) and extract the most relevant features. \gls{NN}$_2$ aims to extract a non-metabolite corrected signal from the low-dimensional representation output by the first network. \gls{NN}$_3$ metabolite corrects the non-metabolite corrected signal.

        All models use a novel activation function, defined as $PSoftplus = \log\big(\mathrm{e}^{x} + \alpha^2 \big)$, where $x$ is the output from the previous layer and $\alpha$ is a learnt parameter which is initialised as $\alpha = 0$. Here, each layer has one $\alpha$ in order to minimise the total number of parameters. This activation function was designed as a fully differentiable drop-in replacement for \textit{PReLU} \cite{Ciuparu2020Soft++Architectures}. The initial value of $\alpha$ is selected such that the activation is initially linear (thus making the model easier to train at early iterations) and becomes non-linear as training progresses.
        % https://www.sciencedirect.com/science/article/pii/S0925231219317163

        Each model, at the point where the spatial dimension is lowest (i.e. at the latent layer), uses a structure similar to a variational autoencoder (predicts a mean and standard deviation of a Gaussian Distribution). Moreover, in order to try to enforce disentanglement and continuity of this layer, a regularisation term to enforce the orthogonality of its output was used. To promote stability of the optimisation, a regularisation term which compares the area under the curve of the prediction and true signals was used in each \gls{NN}.

        Common to all \glspl{NN}, each model was optimised using AdaBelief~\cite{Zhuang2020AdaBeliefGradients} with the warm-up proportion equivalent to one-tenth of the total number of epochs, after which it does not decrease. Weight decay was used to regularise against large weights. Each model was trained initially using \gls{RMSE}, where the mean output of the model is penalised against the expected value and the standard deviation output of the model is encouraged to be close to $1$. Subsequently, a second training regime was used where the negative \gls{LL} function is used to fine-tune the parameters. Gradient accumulation was used to allow for only one data-point to be loaded onto the \gls{GPU} at any one time and for any arbitrary batch-size to be used. The batch size starts at two and increases to the size of total number of data points, where the batch size doubles as the current epoch number quadruples.

        \subsubsection{\gls{NN}$_1$ Autoencoder} \label{sec:NN1}
            This network consists of three blocks, the downsampling block, the latent layer, and the upsampling block. The downsampling block comprises three convolutions, all with kernel size equal to three, and where the final convolution has a stride of two. Edge padding of one was performed prior to each convolution to maintain the size of the image. The latent block is flanked on either side by two convolutions, with a variational latent layer in the middle, that uses the \textit{reparametrisation-trick} (ref). The upsampling block consists of a transposed convolution and two standard convolutions, all with kernel size equal to three, and where the transposed convolution has a stride of two. The mean and standard deviation of the latent layer and final layer are output from the model and passed onto \gls{NN}$_2$.

            Two downsampling and two upsampling blocks were used in this work with filter sizes of eight and sixty-four respectively. The latent layer bottlenecks the number of filters a further four times to sixteen. Borrowed from transformers, a positional encoding was summed with the input image based on its time point. Each time point was treated as an independent training example.

            The target images were smoothed using a Gaussian filter with a \gls{FWHM} equal to three times the voxel size in each dimension. All images were padded such that the size of each dimension was equal to the next larger power of two. Both input and target images were standardised separately, based on parameters obtained from the training set.

        \subsubsection{\gls{NN}$_2$ Signal Extractor} \label{sec:NN2}
            This network consists of three blocks, the downsampling block, the latent layer and the fully connected block. The downsampling block follows the same structure as in \gls{NN}$_1$. The fully connected block consists solely of one fully connected layer.

            The number of downsampling blocks was chosen such that after downsampling one of the spatial dimensions became of size one. The number of filters doubled each time the spatial dimensions decreased by half. After the latent layer, the clinical features are concatenated to its output. The size of the clinical features is increased to the size of the latent layer using a number of fully connected layers where the size doubles at each layer. The number of fully connected layers is determined by the number required to reduce the size of the output to the size of the signal when reducing the number of units by half each time. Positional encoding was used as in \gls{NN}$_1$. All time points were used simultaneously, where the same convolutions are applied independently on each time point before global average pooling and flattening.

            Both input and target data were standardised separately, based on parameters obtained from the training set. Each element of the clinical features was either standardised or encoded using one hot encoding depending on the nature of the feature. Because the autoencoder outputs both a mean and a standard deviation, values were sampled at the input of the network using the same \textit{reparametrisation-trick} as for \gls{NN}$_1$.

        \subsubsection{\gls{NN}$_3$ Metabolite Correction and Reshaping} \label{sec:NN3}
            This network consists solely of fully connected layers. If the network is to metabolite-correct a signal (e.g. from \gls{AIF} or \gls{IDIF}), it takes that signal as input together with the clinical features (age, sex, genotype, injected dose, clinical population) and uses a number of independent fully connected layers to increase their size until they are equal. Then, these two inputs are concatenated together and reduced back to the size of the original signal.

            If the network is instead to correct a signal obtained with \gls{NN}$_2$, both the mean and the standard deviation of the uncorrected signal are input to \gls{NN}$_3$, in addition to the latent layer from \gls{NN}$_2$. % The latent layer is again downsampled as in the previous method, however this time it is possible that features directly related to metabolite correction may be extracted.

            The input data was preprocessed in the same way as for \gls{NN}$_2$.

            %Both input and target data are standardised separately, based on parameters obtained from the training set. Only the mean was standardised. Each element of the clinical features was either standardised or encoded using one hot encoding depending on the nature of the feature. Because the autoencoder outputs both a mean and a standard deviation values were sampled at the input of the network using the same \textit{reparametrisation-trick} as for a variational autoencoder.

    \vspace{-0.5cm}
    
    \subsection{Evaluation}\label{sec:evaluation}
        %The mean and standard deviation were sampled $32$ times (to satisfy the central limit theorem) from the model and the output was sampled $32$ times from the mean of these to acquire multiple realisations of the estimated signal. 
        The model was sampled $32$ times (to satisfy the central limit theorem) resulting in multiple realisations of the estimated signal. 
        Then, candidate signals from~\Fref{sec:candidates} were compared in terms of \glspl{VT}, calculated via Logan graphical method, for each realisation of the output (\Fref{sec:metrics}).
        
        To define the target tissue regions, $69$ \glspl{ROI} were extracted ($48$ cortical and $21$ sub-cortical) in \gls{MNI} space using the Harvard-Oxford Atlas. For every subject, dynamic \gls{PET} images were non-linearly transformed to \gls{MNI} space and smoothed using a \SI{6}{\milli\meter} \gls{FWHM} Gaussian kernel as described in~\cite{Alshelh2020In-vivoIllness}.

        \subsubsection{Metrics} \label{sec:metrics}
         $V_{\mathrm{T}} \in \mathbb{R}^{r \times s \times b}$ were computed, where $r$ is the number of \glspl{ROI} ($r = 69$), $s$ is the number of subjects ($s = 5$) and $b$ is the number of model samples ($b = 32$). For both AIF and IDIF, $b=1$. For the \gls{NN}-based methods, \glspl{VT} were computed for each model realisation and then used to calculate the mean \gls{VT} and its standard deviation.
        Moreover, a \gls{CV} was defined as $\mathrm{CV} = \mathrm{std} ( Pred \; V_{\mathrm{T}} ) / True \; V_{\mathrm{T}}$ with $\mathrm{CV} \in \mathbb{R}^{r \times s }$.

        \subsubsection{Statistical Analyses} \label{sec:stats}
        For each candidate signal, correlation analyses were performed to compare the \glspl{VT} (computed for all the \glspl{ROI} and for all the subjects in the test-set) to the ones obtained with the ground truth signal (\textbf{TRUE-AIF}, see~\Fref{sec:dataproc}). To measure the accuracy of the prediction, the angle between the regression- and the identity-line was also computed, defined as $\theta = 45 - \arctan(m)*180/\pi$, with $m$ being the slope of the regression-line.
        Furthermore, \glspl{CV} were averaged across \glspl{ROI} for each subject of the test-set and compared via a paired t-test for each of the three \gls{NN}-based methods.
       
        \subsubsection{Candidate signals} \label{sec:candidates}
            \begin{itemize}
                \item \textbf{\gls{NN}-\gls{AIF}} - metabolite-corrected \gls{AIF} obtained from (uncorrected) arterial plasma input to \gls{NN}$_3$~\Fref{sec:NN3}.
                \item \textbf{\gls{NN}-\gls{AE}\gls{IF}} - metabolite-corrected \gls{IF} obtained from dynamic \gls{PET} images input to \gls{NN}$_{1,2,3}$~\Fref{sec:NN1} - \Fref{sec:NN3}.
                \item \textbf{\gls{NN}-\gls{IDIF}} - metabolite-corrected \gls{IF} obtained from \gls{IDIF} input to \gls{NN}$_3$~\Fref{sec:NN3}.
                \item  \textbf{\gls{IDIF}} - generated as in~\Fref{sec:dataproc}.
            \end{itemize}
